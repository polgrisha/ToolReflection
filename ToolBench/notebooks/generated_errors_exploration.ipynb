{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_ import make_toolbench_request, detect_error\n",
    "import json\n",
    "from error_explainer import ErrorDetector\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'v4_contrastive'\n",
    "data_path = f'data/toolbench_new_1311/self_correction/generated_errors_no_undetectable_errors_{version}.json'\n",
    "generated_data = json.load(open(data_path))\n",
    "generated_data_2 = json.load(open('data/toolbench_new_1311/self_correction/generated_errors_no_undetectable_errors_v_contrastive_pt2.json'))\n",
    "generated_data = generated_data + generated_data_2\n",
    "generated_data_3 = json.load(open('data/toolbench_new_1311/self_correction/generated_errors_no_undetectable_errors_v_contrastive_pt3.json'))\n",
    "generated_data = generated_data + generated_data_3\n",
    "generated_data_5 = json.load(open('data/toolbench_new_1311/self_correction/generated_errors_no_undetectable_errors_v_contrastive_pt5.json'))\n",
    "generated_data = generated_data + generated_data_5\n",
    "generated_data_6 = json.load(open('data/toolbench_new_1311/self_correction/generated_errors_no_undetectable_errors_v_contrastive_pt6.json'))\n",
    "generated_data = generated_data + generated_data_6\n",
    "generated_data_7 = json.load(open('data/toolbench_new_1311/self_correction/generated_errors_no_undetectable_errors_v_contrastive_pt7.json'))\n",
    "generated_data = generated_data + generated_data_7\n",
    "generated_data_8 = json.load(open('data/toolbench_new_1311/self_correction/generated_errors_no_undetectable_errors_v_contrastive_pt8.json'))\n",
    "generated_data = generated_data + generated_data_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(generated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_error_detector = ErrorDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data_to_assess = []\n",
    "\n",
    "for i, item in enumerate(tqdm(generated_data)):\n",
    "    try:\n",
    "        for step in item['predictions']:\n",
    "            idx = step['chain_idx']\n",
    "            gt = step['ground_truth']\n",
    "            predictions = step['prediction']\n",
    "            incorrect_predictions = []\n",
    "            for pred in predictions:\n",
    "                if pred['function_call']['name'] != 'Finish':\n",
    "                    if pred['function_call']['name'] != gt['function_call']['name']:\n",
    "                        incorrect_predictions.append(pred)\n",
    "                    else:\n",
    "                        try:\n",
    "                            if json.loads(pred['function_call']['arguments']) != json.loads(gt['function_call']['arguments']):\n",
    "                                incorrect_predictions.append(pred)\n",
    "                        except:\n",
    "                            incorrect_predictions.append(pred)\n",
    "            if len(incorrect_predictions) == 2:\n",
    "                if (incorrect_predictions[0]['function_call']['name'] == incorrect_predictions[1]['function_call']['name'] and \n",
    "                    incorrect_predictions[0]['function_call']['arguments'] == incorrect_predictions[1]['function_call']['arguments']):\n",
    "                    incorrect_predictions = incorrect_predictions[:1]\n",
    "            for pred in incorrect_predictions:\n",
    "                full_prediction_text = f\"\\nThought: {pred['content']}\\nAction: {pred['function_call']['name']}\\nAction Input: {pred['function_call']['arguments']}\"\n",
    "                full_gt_text = f\"\\nThought: {gt['content']}\\nAction: {gt['function_call']['name']}\\nAction Input: {gt['function_call']['arguments']}\"\n",
    "                curr_chain = item['conversations'][:idx+2]\n",
    "                gt_tool_call_result = make_toolbench_request(gt['function_call']['name'], gt['function_call']['arguments'])\n",
    "                tool_call_result = make_toolbench_request(pred['function_call']['name'], pred['function_call']['arguments'])\n",
    "                curr_chain.append({'from': 'assistant',\n",
    "                                'value': full_prediction_text})\n",
    "                curr_chain.append({'from': 'function',\n",
    "                                'value': tool_call_result[0]})\n",
    "                if gt_tool_call_result[1] == 0 and not detect_error(gt_tool_call_result[0]):\n",
    "                    if tool_call_result[1] != 0 or detect_error(tool_call_result[0]):\n",
    "                        generated_data_to_assess.append(curr_chain)\n",
    "                    else:\n",
    "                        if llama_error_detector.predict(full_prediction_text, tool_call_result[0]) and not llama_error_detector.predict(full_gt_text, gt_tool_call_result[0]):\n",
    "                            generated_data_to_assess.append(curr_chain)\n",
    "    except Exception as e:\n",
    "        print(e, print(i))\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_prompt =  \"Tool call returned an error. To correct it I need to do the following. \"\n",
    "correction_data = []\n",
    "\n",
    "for curr_item in generated_data_to_assess:\n",
    "    item = copy.deepcopy(curr_item)\n",
    "    prompt_chain = item[:-4]\n",
    "    incorrect_tool_call = item[-2]\n",
    "    incorrect_tool_response = item[-1]\n",
    "    prompt_chain.append(incorrect_tool_call)\n",
    "    prompt_chain.append(incorrect_tool_response)\n",
    "    correct_tool_call = item[-4]\n",
    "    correct_tool_response = item[-3]\n",
    "    correct_tool_call['value'] = \"\\nThought: \" + correction_prompt + correct_tool_call['value'].split(\"\\nThought: \")[1]\n",
    "    prompt_chain.append(correct_tool_call)\n",
    "    prompt_chain.append(correct_tool_response)\n",
    "    if incorrect_tool_response != correct_tool_response:\n",
    "        correction_data.append(prompt_chain)\n",
    "    # correction_data.append(prompt_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(correction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'all_v3'\n",
    "json.dump(correction_data, open(f'data/toolbench_new_1311/self_correction/self_correction_data_no_undetectable_errors_{version}.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
