{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "import sys\n",
    "sys.path.append(\"ToolAlpaca/\")\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from pprint import pprint\n",
    "from agent.custom_parser import CustomMRKLOutputParser2, CustomMRKLOutputParser, CustomMRKLOutputParser3\n",
    "from copy import deepcopy\n",
    "from utils import load_openapi_spec, escape\n",
    "from agent.tools import Tool, GetDetailsTool, tool_projection\n",
    "from agent.tools import CustomInvalidTool\n",
    "from transformers import StoppingCriteria\n",
    "import torch\n",
    "from langchain.schema import AgentAction, AgentFinish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('data/train_data_val_part', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'vicuna-7b-toolalpaca_train_on_train_part/checkpoint-54'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_dir, trust_remote_code=True).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopSequenceCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_sequences, tokenizer):\n",
    "        if isinstance(stop_sequences, str):\n",
    "            stop_sequences = [stop_sequences]\n",
    "        self.stop_sequences = stop_sequences\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs) -> bool:\n",
    "        decoded_output = self.tokenizer.decode(input_ids.tolist()[0])\n",
    "        return any(\n",
    "            decoded_output.endswith(stop_sequence)\n",
    "            for stop_sequence in self.stop_sequences\n",
    "        )\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    "    device='cuda',\n",
    "    do_sample=False,\n",
    "    stopping_criteria=[StopSequenceCriteria('ASSISTANT Observation:', tokenizer)]\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generations = []\n",
    "for item in tqdm(data):\n",
    "    curr_generation = deepcopy(item)\n",
    "    need_to_train = item[1]\n",
    "    curr_prompt = ''\n",
    "    prompts = []\n",
    "    for idx, step in enumerate(item[0]):\n",
    "        if need_to_train[idx]:\n",
    "            prompts.append(curr_prompt)\n",
    "        curr_prompt += step\n",
    "    res = llm.generate(prompts)\n",
    "    generation_idx = 0\n",
    "    for idx in range(len(curr_generation[0])):\n",
    "        if curr_generation[1][idx]:\n",
    "            curr_generation[0][idx] = res.generations[generation_idx][0].text.split('\\nASSISTANT Observation:')[0]\n",
    "            generation_idx += 1\n",
    "    generations.append(curr_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dump(generations, open('../data/test_generations_val_model_trained_on_half_train.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_examples = json.load(open('data/correction_examples_val_model_trained_on_half_train.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_examples = json.load(open('data/test_generations_model_trained_on_half_train.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
